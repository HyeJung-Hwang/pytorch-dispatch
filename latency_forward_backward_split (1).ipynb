{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ntO1kHhp948D"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from torch.utils._python_dispatch import TorchDispatchMode\n",
        "from torch.utils._pytree import tree_map\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ8DXAdx-_fG",
        "outputId": "3293ed63-0ee8-4a16-e0d9-f00dc4110065"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measuring latency without Torch Dispatcher"
      ],
      "metadata": {
        "id": "SH_Ny7zD-BV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.randn(1, 3, 224, 224, device=device)\n",
        "mod = models.resnet50().to(device)\n",
        "optimizer = torch.optim.Adam(mod.parameters(), lr=0.001)\n",
        "\n",
        "total_start = time.time()\n",
        "\n",
        "print(\"=================== Forward =====================\")\n",
        "torch.cuda.synchronize()\n",
        "start_forward = time.time()\n",
        "optimizer.zero_grad()\n",
        "outputs = mod(inp)\n",
        "torch.cuda.synchronize()\n",
        "end_forward = time.time()\n",
        "forward_latency = end_forward - start_forward\n",
        "print(f\"Forward pass latency: {forward_latency} seconds\")\n",
        "\n",
        "print(\"=================== Backward =====================\")\n",
        "start_backward = time.time()\n",
        "loss = outputs.sum()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "torch.cuda.synchronize()\n",
        "end_backward = time.time()\n",
        "backward_latency = end_backward - start_backward\n",
        "print(f\"Backward pass latency: {backward_latency} seconds\")\n",
        "\n",
        "total_end = time.time()\n",
        "total_latency = total_end - total_start\n",
        "print(f\"Total latency (forward + backward): {total_latency} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLn9VvC0-FUU",
        "outputId": "68da642b-c4f8-44fd-ef46-b90216058d88"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================== Forward =====================\n",
            "Forward pass latency: 1.48537015914917 seconds\n",
            "=================== Backward =====================\n",
            "Backward pass latency: 0.4989480972290039 seconds\n",
            "Total latency (forward + backward): 1.9885234832763672 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "num_batches = 10\n",
        "\n",
        "data = [torch.randn(batch_size, 3, 224, 224, device=device) for _ in range(num_batches)]\n",
        "forward_latency_list, backward_latency_list, total_latency_list = [],[],[]\n",
        "\n",
        "for i, batch in enumerate(data):\n",
        "  if i == 0:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = mod(inp)\n",
        "    loss = outputs.sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  else:\n",
        "    total_start = time.time()\n",
        "    print(f\"=================== Batch {i+1} =====================\")\n",
        "    print()\n",
        "    torch.cuda.synchronize()\n",
        "    start_forward = time.time()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = mod(inp)\n",
        "    torch.cuda.synchronize()\n",
        "    end_forward = time.time()\n",
        "    forward_latency = end_forward - start_forward\n",
        "\n",
        "    start_backward = time.time()\n",
        "    loss = outputs.sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    torch.cuda.synchronize()\n",
        "    end_backward = time.time()\n",
        "    backward_latency = end_backward - start_backward\n",
        "\n",
        "    total_end = time.time()\n",
        "    total_latency = total_end - total_start\n",
        "    print(f\"Total latency (forward + backward): {total_latency} seconds\")\n",
        "    print()\n",
        "    print(f\"Forward pass latency: {forward_latency} seconds\")\n",
        "    print(f\"Backward pass latency: {backward_latency} seconds\")\n",
        "    print()\n",
        "\n",
        "    forward_latency_list.append(forward_latency)\n",
        "    backward_latency_list.append(backward_latency)\n",
        "    total_latency_list.append(total_latency)\n",
        "\n",
        "avg_forward_latency = sum(forward_latency_list) / len(forward_latency_list)\n",
        "avg_backward_latency = sum(backward_latency_list) / len(backward_latency_list)\n",
        "avg_total_latency = sum(total_latency_list) / len(total_latency_list)\n",
        "\n",
        "print()\n",
        "print(f\"Average forward pass latency: {avg_forward_latency} seconds\")\n",
        "print(f\"Average backward pass latency: {avg_backward_latency} seconds\")\n",
        "print(f\"Average total latency: {avg_total_latency} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir0yedCdHhpG",
        "outputId": "e155903a-6a6a-4d10-c537-084f64a9ab82"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================== Batch 2 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.10123538970947266 seconds\n",
            "\n",
            "Forward pass latency: 0.06992101669311523 seconds\n",
            "Backward pass latency: 0.026890993118286133 seconds\n",
            "\n",
            "=================== Batch 3 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.09580230712890625 seconds\n",
            "\n",
            "Forward pass latency: 0.06900930404663086 seconds\n",
            "Backward pass latency: 0.02559208869934082 seconds\n",
            "\n",
            "=================== Batch 4 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.09996986389160156 seconds\n",
            "\n",
            "Forward pass latency: 0.07565164566040039 seconds\n",
            "Backward pass latency: 0.024244308471679688 seconds\n",
            "\n",
            "=================== Batch 5 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.2789487838745117 seconds\n",
            "\n",
            "Forward pass latency: 0.254274845123291 seconds\n",
            "Backward pass latency: 0.024589061737060547 seconds\n",
            "\n",
            "=================== Batch 6 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.10617828369140625 seconds\n",
            "\n",
            "Forward pass latency: 0.07870244979858398 seconds\n",
            "Backward pass latency: 0.02599477767944336 seconds\n",
            "\n",
            "=================== Batch 7 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.10424971580505371 seconds\n",
            "\n",
            "Forward pass latency: 0.07685518264770508 seconds\n",
            "Backward pass latency: 0.02604508399963379 seconds\n",
            "\n",
            "=================== Batch 8 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.10181689262390137 seconds\n",
            "\n",
            "Forward pass latency: 0.07651281356811523 seconds\n",
            "Backward pass latency: 0.025229454040527344 seconds\n",
            "\n",
            "=================== Batch 9 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.0973806381225586 seconds\n",
            "\n",
            "Forward pass latency: 0.07306051254272461 seconds\n",
            "Backward pass latency: 0.023458242416381836 seconds\n",
            "\n",
            "=================== Batch 10 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.10449028015136719 seconds\n",
            "\n",
            "Forward pass latency: 0.0797271728515625 seconds\n",
            "Backward pass latency: 0.02469658851623535 seconds\n",
            "\n",
            "\n",
            "Average forward pass latency: 0.09485721588134766 seconds\n",
            "Average backward pass latency: 0.02519339985317654 seconds\n",
            "Average total latency: 0.1211191283331977 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measuring latency with Torch Dispatcher"
      ],
      "metadata": {
        "id": "ZoP_-rmm_eV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_tuple(x):\n",
        "    if not isinstance(x, tuple):\n",
        "        return (x,)\n",
        "    return x\n",
        "\n",
        "class LatencyMeasurementMode(TorchDispatchMode):\n",
        "    def __init__(self, module=None):\n",
        "        self.latency_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
        "        self.parents = ['Global']\n",
        "        if module is not None:\n",
        "            for name, module in dict(module.named_children()).items():\n",
        "                module.register_forward_pre_hook(self.enter_module(name))\n",
        "                module.register_forward_hook(self.exit_module(name))\n",
        "\n",
        "    def enter_module(self, name):\n",
        "        def f(module, inputs):\n",
        "            self.parents.append(name)\n",
        "            inputs = normalize_tuple(inputs)\n",
        "            out = self.create_backwards_pop(name)(*inputs)\n",
        "            return out\n",
        "\n",
        "        return f\n",
        "\n",
        "    def exit_module(self, name):\n",
        "        def f(module, inputs, outputs):\n",
        "            assert self.parents[-1] == name\n",
        "            self.parents.pop()\n",
        "            outputs = normalize_tuple(outputs)\n",
        "            return self.create_backwards_push(name)(*outputs)\n",
        "        return f\n",
        "\n",
        "    def create_backwards_push(self, name):\n",
        "        class PushState(torch.autograd.Function):\n",
        "            @staticmethod\n",
        "            def forward(ctx, *args):\n",
        "                args = tree_map(lambda x: x.clone() if isinstance(x, torch.Tensor) else x, args)\n",
        "                if len(args) == 1:\n",
        "                    return args[0]\n",
        "                return args\n",
        "\n",
        "            @staticmethod\n",
        "            def backward(ctx, *grad_outs):\n",
        "                self.parents.append(name)\n",
        "                return grad_outs\n",
        "\n",
        "        return PushState.apply\n",
        "\n",
        "    def create_backwards_pop(self, name):\n",
        "        class PopState(torch.autograd.Function):\n",
        "            @staticmethod\n",
        "            def forward(ctx, *args):\n",
        "                args = tree_map(lambda x: x.clone() if isinstance(x, torch.Tensor) else x, args)\n",
        "                if len(args) == 1:\n",
        "                    return args[0]\n",
        "                return args\n",
        "\n",
        "            @staticmethod\n",
        "            def backward(ctx, *grad_outs):\n",
        "                assert self.parents[-1] == name\n",
        "                self.parents.pop()\n",
        "                return grad_outs\n",
        "\n",
        "        return PopState.apply\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.latency_counts.clear()\n",
        "        super().__enter__()\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "\n",
        "        ## Added for popping last layer during backward.\n",
        "        # print(\"Pop:\", self.parents.pop())\n",
        "        print()\n",
        "\n",
        "        ## Added for logging forward & backward latency seperately.\n",
        "        self.total_forward_latency = sum(self.latency_counts['Global']['forward'].values())\n",
        "        self.total_backward_latency = sum(self.latency_counts['Global']['backward'].values())\n",
        "\n",
        "        self.total_latency = sum(self.latency_counts['Global'][\"total\"].values())\n",
        "\n",
        "        print(f\"Total latency (forward + backward): {self.total_latency} seconds\")\n",
        "        print()\n",
        "        # print(\"=================== Latency per training steps ===================\")\n",
        "        print(f\"Forward pass latency: {self.total_forward_latency} seconds\")\n",
        "        print(f\"Backward pass latency: {self.total_backward_latency} seconds\")\n",
        "        print()\n",
        "\n",
        "        # print(\"=================== Latency per model modules  ===================\")\n",
        "\n",
        "        # for mod in self.latency_counts.keys():\n",
        "        #     print(f\"Module: \", mod)\n",
        "        #     for phase in ['forward', 'backward']:\n",
        "        #         for k, v in self.latency_counts[mod][phase].items():\n",
        "        #             print(f\"{phase} {k} latency: {v} seconds\")\n",
        "        #     print()\n",
        "\n",
        "        super().__exit__(*args)\n",
        "\n",
        "    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n",
        "        kwargs = kwargs if kwargs else {}\n",
        "        torch.cuda.synchronize()\n",
        "        start_time = time.time()\n",
        "        out = func(*args, **kwargs)\n",
        "        torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "        latency = end_time - start_time\n",
        "\n",
        "        func_packet = func._overloadpacket\n",
        "\n",
        "        ## Added for checking the module if it is forward or backward.(GPT-4)\n",
        "        ## 1)\n",
        "        # current_phase = 'backward'\n",
        "        # if all(isinstance(arg, torch.Tensor) and arg.grad_fn is None for arg in args):\n",
        "        #     current_phase = 'forward'\n",
        "\n",
        "        ## 2)\n",
        "        current_phase = 'backward'\n",
        "        if torch.is_grad_enabled():\n",
        "            current_phase = 'forward'\n",
        "\n",
        "        for par in self.parents:\n",
        "            self.latency_counts[par][\"total\"][func_packet] += latency\n",
        "            self.latency_counts[par][current_phase][func_packet] += latency\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "YrICjGcn_hfI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.randn(1, 3, 224, 224, device=device)\n",
        "mod = models.resnet50().to(device)\n",
        "optimizer = torch.optim.Adam(mod.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "latency_counter = LatencyMeasurementMode(mod)\n",
        "\n",
        "with latency_counter:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = mod(inp)\n",
        "    loss = outputs.sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "Oggd5exz_keX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latency_counter = LatencyMeasurementMode(mod)\n",
        "\n",
        "forward_latency_list, backward_latency_list, total_latency_list = [],[],[]\n",
        "\n",
        "for i, batch in enumerate(data):\n",
        "  if i == 0:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = mod(inp)\n",
        "    loss = outputs.sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  else:\n",
        "    print(f\"=================== Batch {i+ 1} =====================\")\n",
        "    latency_counter = LatencyMeasurementMode(mod)\n",
        "    with latency_counter:\n",
        "      optimizer.zero_grad()\n",
        "      outputs = mod(inp)\n",
        "      loss = outputs.sum()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    forward_latency_list.append(latency_counter.total_forward_latency)\n",
        "    backward_latency_list.append(latency_counter.total_backward_latency)\n",
        "    total_latency_list.append(latency_counter.total_latency)\n",
        "\n",
        "avg_forward_latency = sum(forward_latency_list) / len(forward_latency_list)\n",
        "avg_backward_latency = sum(backward_latency_list) / len(backward_latency_list)\n",
        "avg_total_latency = sum(total_latency_list) / len(total_latency_list)\n",
        "\n",
        "print(f\"Average forward pass latency: {avg_forward_latency} seconds\")\n",
        "print(f\"Average backward pass latency: {avg_backward_latency} seconds\")\n",
        "print(f\"Average total latency: {avg_total_latency} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4E06LpfIWof",
        "outputId": "852697c2-4e4e-4d83-dbcb-d4de526c623e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================== Batch 2 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.20812749862670898 seconds\n",
            "\n",
            "Forward pass latency: 0.05593228340148926 seconds\n",
            "Backward pass latency: 0.15219521522521973 seconds\n",
            "\n",
            "=================== Batch 3 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.10734891891479492 seconds\n",
            "\n",
            "Forward pass latency: 0.0227658748626709 seconds\n",
            "Backward pass latency: 0.08458304405212402 seconds\n",
            "\n",
            "=================== Batch 4 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.10840916633605957 seconds\n",
            "\n",
            "Forward pass latency: 0.022467851638793945 seconds\n",
            "Backward pass latency: 0.08594131469726562 seconds\n",
            "\n",
            "=================== Batch 5 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.1025848388671875 seconds\n",
            "\n",
            "Forward pass latency: 0.021711111068725586 seconds\n",
            "Backward pass latency: 0.08087372779846191 seconds\n",
            "\n",
            "=================== Batch 6 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.1057281494140625 seconds\n",
            "\n",
            "Forward pass latency: 0.0227506160736084 seconds\n",
            "Backward pass latency: 0.0829775333404541 seconds\n",
            "\n",
            "=================== Batch 7 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.11074233055114746 seconds\n",
            "\n",
            "Forward pass latency: 0.023838043212890625 seconds\n",
            "Backward pass latency: 0.08690428733825684 seconds\n",
            "\n",
            "=================== Batch 8 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.10796976089477539 seconds\n",
            "\n",
            "Forward pass latency: 0.022174835205078125 seconds\n",
            "Backward pass latency: 0.08579492568969727 seconds\n",
            "\n",
            "=================== Batch 9 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.1209251880645752 seconds\n",
            "\n",
            "Forward pass latency: 0.02376532554626465 seconds\n",
            "Backward pass latency: 0.09715986251831055 seconds\n",
            "\n",
            "=================== Batch 10 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.1389777660369873 seconds\n",
            "\n",
            "Forward pass latency: 0.025863170623779297 seconds\n",
            "Backward pass latency: 0.11311459541320801 seconds\n",
            "\n",
            "Average forward pass latency: 0.026807679070366755 seconds\n",
            "Average backward pass latency: 0.09661605623033312 seconds\n",
            "Average total latency: 0.12342373530069987 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VcBgdIErOauh"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}