{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ntO1kHhp948D"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from torch.utils._python_dispatch import TorchDispatchMode\n",
        "from torch.utils._pytree import tree_map\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ8DXAdx-_fG",
        "outputId": "3293ed63-0ee8-4a16-e0d9-f00dc4110065"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measuring latency without Torch Dispatcher"
      ],
      "metadata": {
        "id": "SH_Ny7zD-BV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.randn(1, 3, 224, 224, device=device)\n",
        "mod = models.resnet50().to(device)\n",
        "optimizer = torch.optim.Adam(mod.parameters(), lr=0.001)\n",
        "\n",
        "total_start = time.time()\n",
        "\n",
        "print(\"=================== Forward =====================\")\n",
        "torch.cuda.synchronize()\n",
        "start_forward = time.time()\n",
        "optimizer.zero_grad()\n",
        "outputs = mod(inp)\n",
        "torch.cuda.synchronize()\n",
        "end_forward = time.time()\n",
        "forward_latency = end_forward - start_forward\n",
        "print(f\"Forward pass latency: {forward_latency} seconds\")\n",
        "\n",
        "print(\"=================== Backward =====================\")\n",
        "start_backward = time.time()\n",
        "loss = outputs.sum()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "torch.cuda.synchronize()\n",
        "end_backward = time.time()\n",
        "backward_latency = end_backward - start_backward\n",
        "print(f\"Backward pass latency: {backward_latency} seconds\")\n",
        "\n",
        "total_end = time.time()\n",
        "total_latency = total_end - total_start\n",
        "print(f\"Total latency (forward + backward): {total_latency} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLn9VvC0-FUU",
        "outputId": "68da642b-c4f8-44fd-ef46-b90216058d88"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================== Forward =====================\n",
            "Forward pass latency: 1.48537015914917 seconds\n",
            "=================== Backward =====================\n",
            "Backward pass latency: 0.4989480972290039 seconds\n",
            "Total latency (forward + backward): 1.9885234832763672 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "num_batches = 10\n",
        "\n",
        "data = [torch.randn(batch_size, 3, 224, 224, device=device) for _ in range(num_batches)]\n",
        "forward_latency_list, backward_latency_list, total_latency_list = [],[],[]\n",
        "\n",
        "for i, batch in enumerate(data):\n",
        "  if i == 0:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = mod(inp)\n",
        "    loss = outputs.sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  else:\n",
        "    total_start = time.time()\n",
        "    print(f\"=================== Batch {i+1} =====================\")\n",
        "    print()\n",
        "    torch.cuda.synchronize()\n",
        "    start_forward = time.time()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = mod(inp)\n",
        "    torch.cuda.synchronize()\n",
        "    end_forward = time.time()\n",
        "    forward_latency = end_forward - start_forward\n",
        "\n",
        "    start_backward = time.time()\n",
        "    loss = outputs.sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    torch.cuda.synchronize()\n",
        "    end_backward = time.time()\n",
        "    backward_latency = end_backward - start_backward\n",
        "\n",
        "    total_end = time.time()\n",
        "    total_latency = total_end - total_start\n",
        "    print(f\"Total latency (forward + backward): {total_latency} seconds\")\n",
        "    print()\n",
        "    print(f\"Forward pass latency: {forward_latency} seconds\")\n",
        "    print(f\"Backward pass latency: {backward_latency} seconds\")\n",
        "    print()\n",
        "\n",
        "    forward_latency_list.append(forward_latency)\n",
        "    backward_latency_list.append(backward_latency)\n",
        "    total_latency_list.append(total_latency)\n",
        "\n",
        "avg_forward_latency = sum(forward_latency_list) / len(forward_latency_list)\n",
        "avg_backward_latency = sum(backward_latency_list) / len(backward_latency_list)\n",
        "avg_total_latency = sum(total_latency_list) / len(total_latency_list)\n",
        "\n",
        "print()\n",
        "print(f\"Average forward pass latency: {avg_forward_latency} seconds\")\n",
        "print(f\"Average backward pass latency: {avg_backward_latency} seconds\")\n",
        "print(f\"Average total latency: {avg_total_latency} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir0yedCdHhpG",
        "outputId": "937e2a8a-132c-4c63-849d-d0ef6ee1f683"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================== Batch 2 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.09418249130249023 seconds\n",
            "\n",
            "Forward pass latency: 0.05772280693054199 seconds\n",
            "Backward pass latency: 0.026458740234375 seconds\n",
            "\n",
            "=================== Batch 3 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.08638954162597656 seconds\n",
            "\n",
            "Forward pass latency: 0.05787229537963867 seconds\n",
            "Backward pass latency: 0.0274200439453125 seconds\n",
            "\n",
            "=================== Batch 4 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.09773874282836914 seconds\n",
            "\n",
            "Forward pass latency: 0.07194256782531738 seconds\n",
            "Backward pass latency: 0.024062633514404297 seconds\n",
            "\n",
            "=================== Batch 5 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.2598686218261719 seconds\n",
            "\n",
            "Forward pass latency: 0.2314894199371338 seconds\n",
            "Backward pass latency: 0.028301715850830078 seconds\n",
            "\n",
            "=================== Batch 6 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.08493828773498535 seconds\n",
            "\n",
            "Forward pass latency: 0.05778980255126953 seconds\n",
            "Backward pass latency: 0.02482295036315918 seconds\n",
            "\n",
            "=================== Batch 7 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.1203770637512207 seconds\n",
            "\n",
            "Forward pass latency: 0.0813291072845459 seconds\n",
            "Backward pass latency: 0.03786134719848633 seconds\n",
            "\n",
            "=================== Batch 8 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.1294994354248047 seconds\n",
            "\n",
            "Forward pass latency: 0.10011625289916992 seconds\n",
            "Backward pass latency: 0.028914928436279297 seconds\n",
            "\n",
            "=================== Batch 9 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.13368701934814453 seconds\n",
            "\n",
            "Forward pass latency: 0.1014094352722168 seconds\n",
            "Backward pass latency: 0.030407428741455078 seconds\n",
            "\n",
            "=================== Batch 10 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.11176705360412598 seconds\n",
            "\n",
            "Forward pass latency: 0.07889652252197266 seconds\n",
            "Backward pass latency: 0.03236103057861328 seconds\n",
            "\n",
            "\n",
            "Average forward pass latency: 0.09317424562242296 seconds\n",
            "Average backward pass latency: 0.028956757651435003 seconds\n",
            "Average total latency: 0.12427202860514323 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measuring latency with Torch Dispatcher"
      ],
      "metadata": {
        "id": "ZoP_-rmm_eV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_tuple(x):\n",
        "    if not isinstance(x, tuple):\n",
        "        return (x,)\n",
        "    return x\n",
        "\n",
        "class LatencyMeasurementMode(TorchDispatchMode):\n",
        "    def __init__(self, module=None):\n",
        "        self.latency_counts = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
        "        self.parents = ['Global']\n",
        "        if module is not None:\n",
        "            for name, module in dict(module.named_children()).items():\n",
        "                module.register_forward_pre_hook(self.enter_module(name))\n",
        "                module.register_forward_hook(self.exit_module(name))\n",
        "\n",
        "    def enter_module(self, name):\n",
        "        def f(module, inputs):\n",
        "            self.parents.append(name)\n",
        "            inputs = normalize_tuple(inputs)\n",
        "            out = self.create_backwards_pop(name)(*inputs)\n",
        "            return out\n",
        "\n",
        "        return f\n",
        "\n",
        "    def exit_module(self, name):\n",
        "        def f(module, inputs, outputs):\n",
        "            assert self.parents[-1] == name\n",
        "            self.parents.pop()\n",
        "            outputs = normalize_tuple(outputs)\n",
        "            return self.create_backwards_push(name)(*outputs)\n",
        "        return f\n",
        "\n",
        "    def create_backwards_push(self, name):\n",
        "        class PushState(torch.autograd.Function):\n",
        "            @staticmethod\n",
        "            def forward(ctx, *args):\n",
        "                args = tree_map(lambda x: x.clone() if isinstance(x, torch.Tensor) else x, args)\n",
        "                if len(args) == 1:\n",
        "                    return args[0]\n",
        "                return args\n",
        "\n",
        "            @staticmethod\n",
        "            def backward(ctx, *grad_outs):\n",
        "                self.parents.append(name)\n",
        "                return grad_outs\n",
        "\n",
        "        return PushState.apply\n",
        "\n",
        "    def create_backwards_pop(self, name):\n",
        "        class PopState(torch.autograd.Function):\n",
        "            @staticmethod\n",
        "            def forward(ctx, *args):\n",
        "                args = tree_map(lambda x: x.clone() if isinstance(x, torch.Tensor) else x, args)\n",
        "                if len(args) == 1:\n",
        "                    return args[0]\n",
        "                return args\n",
        "\n",
        "            @staticmethod\n",
        "            def backward(ctx, *grad_outs):\n",
        "                assert self.parents[-1] == name\n",
        "                self.parents.pop()\n",
        "                return grad_outs\n",
        "\n",
        "        return PopState.apply\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.latency_counts.clear()\n",
        "        super().__enter__()\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "\n",
        "        ## Added for popping last layer during backward.\n",
        "        # print(\"Pop:\", self.parents.pop())\n",
        "        print()\n",
        "\n",
        "        ## Added for logging forward & backward latency seperately.\n",
        "        self.total_forward_latency = sum(self.latency_counts['Global']['forward'].values())\n",
        "        self.total_backward_latency = sum(self.latency_counts['Global']['backward'].values())\n",
        "\n",
        "        self.total_latency = sum(self.latency_counts['Global'][\"total\"].values())\n",
        "\n",
        "        print(f\"Total latency (forward + backward): {self.total_latency} seconds\")\n",
        "        print()\n",
        "        # print(\"=================== Latency per training steps ===================\")\n",
        "        print(f\"Forward pass latency: {self.total_forward_latency} seconds\")\n",
        "        print(f\"Backward pass latency: {self.total_backward_latency} seconds\")\n",
        "        print()\n",
        "\n",
        "        # print(\"=================== Latency per model modules  ===================\")\n",
        "\n",
        "        # for mod in self.latency_counts.keys():\n",
        "        #     print(f\"Module: \", mod)\n",
        "        #     for phase in ['forward', 'backward']:\n",
        "        #         for k, v in self.latency_counts[mod][phase].items():\n",
        "        #             print(f\"{phase} {k} latency: {v} seconds\")\n",
        "        #     print()\n",
        "\n",
        "        super().__exit__(*args)\n",
        "\n",
        "    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n",
        "        kwargs = kwargs if kwargs else {}\n",
        "        torch.cuda.synchronize()\n",
        "        start_time = time.time()\n",
        "        out = func(*args, **kwargs)\n",
        "        torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "        latency = end_time - start_time\n",
        "\n",
        "        func_packet = func._overloadpacket\n",
        "\n",
        "        ## Added for checking the module if it is forward or backward.(GPT-4)\n",
        "        ## 1)\n",
        "        current_phase = 'backward'\n",
        "        if all(isinstance(arg, torch.Tensor) and arg.grad_fn is None for arg in args):\n",
        "            current_phase = 'forward'\n",
        "\n",
        "        ## 2)\n",
        "        # current_phase = 'backward'\n",
        "        # if torch.is_grad_enabled():\n",
        "        #     current_phase = 'forward'\n",
        "\n",
        "        for par in self.parents:\n",
        "            self.latency_counts[par][\"total\"][func_packet] += latency\n",
        "            self.latency_counts[par][current_phase][func_packet] += latency\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "YrICjGcn_hfI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = torch.randn(1, 3, 224, 224, device=device)\n",
        "mod = models.resnet50().to(device)\n",
        "optimizer = torch.optim.Adam(mod.parameters(), lr=0.001)\n",
        "\n",
        "\n",
        "latency_counter = LatencyMeasurementMode(mod)\n",
        "\n",
        "with latency_counter:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = mod(inp)\n",
        "    loss = outputs.sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "Oggd5exz_keX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latency_counter = LatencyMeasurementMode(mod)\n",
        "\n",
        "forward_latency_list, backward_latency_list, total_latency_list = [],[],[]\n",
        "\n",
        "for i, batch in enumerate(data):\n",
        "  if i == 0:\n",
        "    optimizer.zero_grad()\n",
        "    outputs = mod(inp)\n",
        "    loss = outputs.sum()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  else:\n",
        "    print(f\"=================== Batch {i+ 1} =====================\")\n",
        "    latency_counter = LatencyMeasurementMode(mod)\n",
        "    with latency_counter:\n",
        "      optimizer.zero_grad()\n",
        "      outputs = mod(inp)\n",
        "      loss = outputs.sum()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    forward_latency_list.append(latency_counter.total_forward_latency)\n",
        "    backward_latency_list.append(latency_counter.total_backward_latency)\n",
        "    total_latency_list.append(latency_counter.total_latency)\n",
        "\n",
        "avg_forward_latency = sum(forward_latency_list) / len(forward_latency_list)\n",
        "avg_backward_latency = sum(backward_latency_list) / len(backward_latency_list)\n",
        "avg_total_latency = sum(total_latency_list) / len(total_latency_list)\n",
        "\n",
        "print(f\"Average forward pass latency: {avg_forward_latency} seconds\")\n",
        "print(f\"Average backward pass latency: {avg_backward_latency} seconds\")\n",
        "print(f\"Average total latency: {avg_total_latency} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4E06LpfIWof",
        "outputId": "df1736f2-e86e-4103-b6e9-8dde67bc5d78"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================== Batch 2 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.09601140022277832 seconds\n",
            "\n",
            "Forward pass latency: 0.015318632125854492 seconds\n",
            "Backward pass latency: 0.08069276809692383 seconds\n",
            "\n",
            "=================== Batch 3 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.09161567687988281 seconds\n",
            "\n",
            "Forward pass latency: 0.010662317276000977 seconds\n",
            "Backward pass latency: 0.08095335960388184 seconds\n",
            "\n",
            "=================== Batch 4 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.17192530632019043 seconds\n",
            "\n",
            "Forward pass latency: 0.01895594596862793 seconds\n",
            "Backward pass latency: 0.1529693603515625 seconds\n",
            "\n",
            "=================== Batch 5 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.11286115646362305 seconds\n",
            "\n",
            "Forward pass latency: 0.013003826141357422 seconds\n",
            "Backward pass latency: 0.09985733032226562 seconds\n",
            "\n",
            "=================== Batch 6 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.09781455993652344 seconds\n",
            "\n",
            "Forward pass latency: 0.011237859725952148 seconds\n",
            "Backward pass latency: 0.08657670021057129 seconds\n",
            "\n",
            "=================== Batch 7 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.09904956817626953 seconds\n",
            "\n",
            "Forward pass latency: 0.011846065521240234 seconds\n",
            "Backward pass latency: 0.0872035026550293 seconds\n",
            "\n",
            "=================== Batch 8 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.10404205322265625 seconds\n",
            "\n",
            "Forward pass latency: 0.015480756759643555 seconds\n",
            "Backward pass latency: 0.0885612964630127 seconds\n",
            "\n",
            "=================== Batch 9 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.09978008270263672 seconds\n",
            "\n",
            "Forward pass latency: 0.012042045593261719 seconds\n",
            "Backward pass latency: 0.087738037109375 seconds\n",
            "\n",
            "=================== Batch 10 =====================\n",
            "\n",
            "Total latency (forward + backward): 0.09914088249206543 seconds\n",
            "\n",
            "Forward pass latency: 0.011514902114868164 seconds\n",
            "Backward pass latency: 0.08762598037719727 seconds\n",
            "\n",
            "Average forward pass latency: 0.01334026124742296 seconds\n",
            "Average backward pass latency: 0.0946864816877577 seconds\n",
            "Average total latency: 0.10802674293518066 seconds\n"
          ]
        }
      ]
    }
  ]
}